{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Given the following data:\n",
    "\n",
    "| Tid | Refund | Marital Status | Taxable Income (K) | Cheat |\n",
    "|-----|--------|----------------|--------------------|-------|\n",
    "| 1   | Yes    | Single         | 125                | No    |\n",
    "| 2   | No     | Married        | 100                | No    |\n",
    "| 3   | No     | Single         | 70                 | No    |\n",
    "| 4   | Yes    | Married        | 120                | No    |\n",
    "| 5   | No     | Divorced       | 95                 | Yes   |\n",
    "| 6   | No     | Married        | 60                 | No    |\n",
    "| 7   | Yes    | Divorced       | 220                | No    |\n",
    "| 8   | No     | Single         | 85                 | Yes   |\n",
    "| 9   | No     | Married        | 75                 | No    |\n",
    "| 10  | No     | Single         | 90                 | Yes   |\n",
    "\n",
    "What is the best first split (using Gini - note, for the continuous feature, check quartile boundaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refund - n(4/7) y - 3 pure, ms- single - 2/4 married 4 pure divorce  1/2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the best first split to determine cheat would be using the Marital Status variable and split on the Married Value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Build a decision tree to fit the [federalist papers](https://www.kaggle.com/datasets/tobyanderson/federalist-papers_) data, available in the data directory (click on the link to find out more information about this data). Note that you should restrict your analysis to papers by Hamilton or Madison.  Plot your training and test scores to pick a value for ccp_alpha. What did you pick?  Run your trained classifier on the \"disputed\" papers.  What does your model tell you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yr/ck3hvtw16ks5djc79x7fgf6r0000gn/T/ipykernel_27674/2725781921.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  hm_df = hm_df.append(df[df[\"author\"].values == 'Madison'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author      a    all   also     an    and    any    are     as     at  ...  \\\n",
       "0       1  0.213  0.083  0.000  0.083  0.343  0.056  0.111  0.093  0.065  ...   \n",
       "1       1  0.369  0.070  0.006  0.076  0.411  0.023  0.053  0.117  0.065  ...   \n",
       "2       1  0.305  0.047  0.007  0.068  0.386  0.047  0.102  0.108  0.088  ...   \n",
       "3       1  0.391  0.045  0.015  0.030  0.270  0.045  0.060  0.090  0.015  ...   \n",
       "4       1  0.327  0.096  0.000  0.086  0.356  0.014  0.086  0.072  0.115  ...   \n",
       "\n",
       "     was   were   what   when  which    who   will   with  would   your  \n",
       "0  0.000  0.000  0.000  0.009  0.158  0.074  0.222  0.046  0.019  0.074  \n",
       "1  0.000  0.012  0.012  0.012  0.147  0.029  0.094  0.129  0.270  0.000  \n",
       "2  0.000  0.000  0.007  0.000  0.156  0.007  0.074  0.122  0.149  0.000  \n",
       "3  0.000  0.000  0.000  0.045  0.165  0.045  0.135  0.150  0.210  0.000  \n",
       "4  0.014  0.038  0.014  0.019  0.264  0.029  0.091  0.086  0.062  0.010  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sklearn \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"data/federalistpapers.csv\")\n",
    "df[df[\"author\"].values == 'Hamilton']\n",
    "df[df[\"author\"].values == 'Madison']\n",
    "hm_df = df[df[\"author\"].values == 'Hamilton']\n",
    "hm_df = hm_df.append(df[df[\"author\"].values == 'Madison'])\n",
    "hm_df = hm_df.drop([\"filename\"], axis = 1)\n",
    "hm_df = hm_df.reset_index()\n",
    "hm_df = hm_df.drop([\"index\"], axis = 1)\n",
    "df[df[\"author\"] == 'Madison']\n",
    "hm_df.replace({\"Madison\": 0, \"Hamilton\": 1}, inplace=True)\n",
    "hm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(hm_df, hm_df[\"author\"]):\n",
    "    train_set = hm_df.loc[train_index]\n",
    "    test_set = hm_df.loc[test_index]\n",
    "\n",
    "strat_train = train_set\n",
    "train_y = strat_train[\"author\"]\n",
    "strat_train = strat_train.drop(\"author\", axis=1)\n",
    "strat_test = test_set.copy()\n",
    "test_y = test_set[\"author\"]\n",
    "test_x = test_set.drop(\"author\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51]\n",
      "Fold 1:\n",
      "  Train: index=[ 0  1  2  3  4  5  6  7  8  9 10 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1587\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:945\u001b[0m, in \u001b[0;36mSeries._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:930\u001b[0m, in \u001b[0;36mSeries.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m indices \u001b[38;5;241m=\u001b[39m ensure_platform_int(indices)\n\u001b[0;32m--> 930\u001b[0m new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtake(indices)\n\u001b[1;32m    931\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mtake(indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:1183\u001b[0m, in \u001b[0;36mIndex.take\u001b[0;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1183\u001b[0m     taken \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m   1184\u001b[0m         values, indices, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_na_value\n\u001b[1;32m   1185\u001b[0m     )\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# algos.take passes 'axis' keyword which not all EAs accept\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1577\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1576\u001b[0m     \u001b[38;5;66;03m# NumPy style\u001b[39;00m\n\u001b[0;32m-> 1577\u001b[0m     result \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mtake(indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14 is out of bounds for axis 0 with size 14",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train_error\u001b[38;5;241m.\u001b[39mappend(clf\u001b[38;5;241m.\u001b[39mscore(train, kin_y))\n\u001b[1;32m     13\u001b[0m test \u001b[38;5;241m=\u001b[39m strat_train\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m---> 14\u001b[0m kest_y \u001b[38;5;241m=\u001b[39m test_y\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     15\u001b[0m train_error\u001b[38;5;241m.\u001b[39mappend((clf\u001b[38;5;241m.\u001b[39mscore(train, kin_y)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1616\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_list_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1620\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1590\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[0;32m-> 1590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state= 42)\n",
    "train_error = []\n",
    "test_error = []\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(strat_train)):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     print(f\"  Train: index={train_index}\")\n",
    "#     train = strat_train.iloc[train_index]\n",
    "#     kin_y = train_y.iloc[train_index]\n",
    "#     clf.fit(train, kin_y)\n",
    "#     train_error.append(clf.score(train, kin_y))\n",
    "#     test = strat_train.iloc[test_index]\n",
    "#     kest_y = test_y.iloc[test_index]\n",
    "#     train_error.append((clf.score(train, kin_y)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Build a voting classifier for the federalist papers, using all of the non-ensemble methods you've been exposed to in this class thus far (i.e., KNN, SVM, logistic regression, naive bayes, SGDClassifier, decision tree).\n",
    "\n",
    "1) Compare this to a RandomForest classifier.  Which works the best?\n",
    "2) Compare this to a GradientBoosting classifier.  Which works the best?\n",
    "3) Add the RandomForest and GradientBoosting classifiers to your voting classifier.  Does you performance improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "When does it make sense to use a Bagging Classifier?  In the following, explore different data parameters to develop your intuition for which classifier makes sense in which situation. \n",
    "\n",
    "1. Gradually increase the noise in the data (using the noise parameter).  How do the different classifiers perform.  Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_train_pred_tree = tree.predict(X_train)\n",
    "print(\"Decision Tree Accuracy (train):\", accuracy_score(y_train, y_train_pred_tree))\n",
    "print(\"Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "y_train_pred_log = log_reg.predict(X_train)\n",
    "print(\"Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_train_pred_log))\n",
    "print(\"Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Now, do the same thing in the following.  What do you notice. How do you explain your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_samples = 100\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_tree = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_tree.fit(X_train, y_train)\n",
    "y_pred_bag_train = bag_tree.predict(X_train)\n",
    "y_pred_bag = bag_tree.predict(X_test)\n",
    "print(\"Bagging Decision Tree Accuracy (train):\", accuracy_score(y_train, y_pred_bag_train))\n",
    "print(\"Bagging Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_bag))\n",
    "\n",
    "\n",
    "bag_log = BaggingClassifier(LogisticRegression(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_log.fit(X_train, y_train)\n",
    "y_pred_bag_log_train = bag_log.predict(X_train)\n",
    "y_pred_bag_log = bag_log.predict(X_test)\n",
    "print(\"Bagging Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_pred_bag_log_train))\n",
    "print(\"Bagging Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_bag_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now go back and start increasing the `max_samples` parameter.  How do things change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "The \"wine\" dataset contains data about the chemical makeup of different varieties of wine and critics scores.  Use XGBoost to build a classifier for this data.  Manually tune the hyperparameters of the XGBoost model to try to achieve better accuracy on the test set than the baseline model. Some hyperparameters to consider tweaking:\n",
    "   - `learning_rate`\n",
    "   - `max_depth`\n",
    "   - `n_estimators`\n",
    "   - `gamma`\n",
    "   - `subsample`\n",
    "   - `colsample_bytree`\n",
    "\n",
    "See [the online docs](https://xgboost.readthedocs.io/en/stable/parameter.html) for more info.\n",
    "\n",
    "After tuning, use the `plot_importance` function again to see if feature importances have changed after tuning.\n",
    "\n",
    "\n",
    "1. How did hyperparameter tuning affect the model's accuracy? Which hyperparameters seemed to have the most influence?\n",
    "2. Did feature importances change after tuning? If so, why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you don't have XGBoost installed\n",
    "%pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "data = load_wine()\n",
    "\n",
    "# We'll use a data frame to make sure we get real feature names out\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='multi:softprob', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "plot_importance(clf)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
